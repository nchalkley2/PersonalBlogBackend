<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!--meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"-->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	
	<!-- favicon shit -->
	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/manifest.json">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
	<meta name="theme-color" content="#ffffff">
    
	<title>Nick's Blog</title>

    <link rel="stylesheet" type="text/css" href="./css/main.css">

	<!-- JQuery lib-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<!-- Latest compiled JavaScript -->
</head>

<body>
    <div style="position:fixed;left:0;top:0;width:100%;height:100%;z-index:-10;">
		<canvas id="confetti" width="1" height="1" style="z-index:-10;display:none;"></canvas>
	</div>
    <script type="text/javascript" src="./js/confetti.js"></script>

	<div class="content">

		<!-- Title post -->
		<div class="postcontainer">
			<div class="post">
				<div class="title">
					<ul class="titletable">
						<li class="tablecell"><a class="title" href="/"><h class="titletext">Nick's Blog</h></a></li>
						<li class="tablecell" style="text-align: center;">
							<div class="titlelink">
								<!--a href="#" onclick="toggleConfetti()">
								<img style="width: 32px;height: 32px;margin-top:4px;" src="https://upload.wikimedia.org/wikipedia/commons/e/ea/Emoji_u1f44c.svg">
								</a>
                                <script>
									function toggleConfetti() {
										var x = document.getElementById("confetti");
										if (x.style.display === "none") {
											x.style.display = "block";
										} else {
											x.style.display = "none";
										}
									}
								</script-->

								<a href="https://github.com/nchalkley2">
									<img style="width: 32px;height: 32px;margin-top:4px;" src="img/github.svg">
								</a>

								<a href="resume.pdf">
									<img style="width: 32px;height: 32px;margin-top:6px;" src="img/cv.svg">
								</a>
							</div>
						</li>
					</ul>
				</div>
				<hr style="margin-bottom:0; margin-top:0; height: 8px">
			</div>
		</div>

		<!--div class="postcontainer">
			<div class="post">
				<div class="post-text">
					<h1>Networking and Photogrammetry for Dummies</h1>
				
					<hr>

					<h2>Background</h2>
					<p class="fp">
						
						<span class="sntc">I’d say that my job responsibilies as a SIF fellow lie somewhere between creating computer generated art and exploring technological innovation, which is precisely why I love this job so much, as these two activities completely align with my interests.</span> 
						<span class="sntc">For years I have been in love with the 3d art field, and I love keeping up with the front line of technological innovation in art, and the most exciting, new thing on the block in computer generated art is Photogrammetry<sup class="citation" href="#cite_note-4"><a href="#cite_note-4">[1]</a></sup>.</span> 
						<span class="sntc">And the way I see it, photogrammetry is the next big step in computer generated graphics.</span> 
						<span class="sntc">Why I believe that it is the next big step in CG graphics is that it greatly reduces the amount of work artists have to do to create realistic 3d assets, and it gives results that simply can't be beat.</span> 
						<span class="sntc">There's no amount of effort that an artist can put in to an asset to match a scan from real life.</span> 
						<span class="sntc">I’ve been keeping up with photogrammetry’s boundary pushes for a while now; I've been watching presentations about it on GDC’s YouTube page.</span> 
						<span class="sntc">So as I've been keeping up with the boundary pushes photogrammetry has made, I’ve seen independent artists on the website Polycount produce unbelievable work using this technique, and in the past few years, i’ve read about household name companies like Epic Games and Dice using photogrammetry in their work flows to easily make the most realistic graphics possible.</span> 
						<span class="sntc">Needless to say, photogrammetry is an incredibly exciting technological innovation for a 3d artist like me.</span> 
                    </p>

                    <p class="fp">
                        <span class="sntc">Until now, I’ve been talking about photogrammetry at length, and you’re probably wondering how my interest in this art form relates to my work at the SIF program.</span> 
					    <span class="sntc">Our latest SIF project, which involves the 3d mapping of Oakland Cemetery, exclusively uses photogrammetry to accomplish its ends.</span> 
					</p>

					<hr>

					<p class="fp">
The Oakland Cemetery project involves using photogrammetry to make a 3d model of the entire cemetery. When I first heard about this project, I was incredibly excited, but then I thought about the sheer amount of work our small team would have to do for the project, and I turned very skeptical. Oakland cemetery covers a whopping 88 acres of land, and we only have a small team working part time to capture the entire cemetery. To say that this task is unfeasible is an understatement. Luckily, we had set up a collaboration with a studio to help us accomplish our project. Which leads me to the second topic covered in this blog post: Networking. We reached out to the studio Beam Imagination for help with our project, and they brought out the big guns.

On the first day of our photoscanning project, they brought a drone to the cemetery to survey the area. By the end of the day, we had already scanned about one-third of the cemetery. So we had our work cut out for us, right? Well, that’s not entirely true. The aerial photoscans provided a good bird's eye view of the cemetery, but it couldn’t capture any fine detail, and the trees covered up most of the cemetery. To get the full scans, we would have to manually do detailed scans with cameras. Luckily Beam Imagination also provided us with very high quality cameras for doing detailed scans. So after the land survey, we decided to sandbox a small area and do A/B testing to determine the best setup for the manual photogrammetry.

We brought out about a hundred thousand dollars worth of equipment to Oakland Cemetery to do the testing. Our lineup of camera bodies included a Sony A7r II, a Nikon D850, and a RED Epic-W. We used an 8mm fisheye lens, an 18mm Zeiss lens, and a 25 mm Zeiss lens with the Sony A7r II, A 14-24mm lens, a 24mm, and a 24-70mm lens with the Nikon, and a 24-70mm lens at 24mm with the RED Epic-W. Agisoft recommends using a 50mm rectilinear (not fisheye) lens to do the scanning, so our lineup choices are a bit oddball. It is even moreso with the RED Epic-W, as Agisoft recommends using still images and the program isn’t designed to work with video. Since we had our equipment lineup for the A/B testing, we just had to do the scanning.
We scanned a statue in our sandbox multiple times with the different camera equipment to do the A/B testing. On the day of the tests, the weather was bright and sunny (Georgia weather sucks for photogrammetry), so we didn’t get the best results. You can notice this in the scans, as the sun is setting pretty rapidly between the different scans.

Once I got all the scanning up, my job was to load the pictures into Agisoft Photoscan and generate a dense point cloud for each set of pictures. The Photoscan program generates quality statistics for each point cloud, so I used those to compare the quality of the scans our different equipment set ups created.

Unfortunately, because of the amount of data we captured, I was not able to compute all of the photogrammetric data for a thorough quality analysis. I was able to get through all of the tests except for the RED Epic-W. I don’t have a computer strong enough to compute the photogrammetric data for every frame, so i’ll have to wait for a few days when I can hog Beam Imagination’s workstation. But according to the data that I did compute, our Test Case 3 had the lowest amount of reprojection error. This test case was composed of 66 images taken with the Sony A7r II and a 18mm lens. Also according to the test data, we had the lowest amount of reprojection error when we corrected for vignetting in the picture, as opposed to no correction and vignette + lens distortion correction. This result matches Dice’s recommendation of only using vignette correction on the photos. 

The next step in our project is a lot less interesting: we have to wait for the weather to get worse. Overcast days are perfect for photogrammetry, and as I mentioned before most Georgia days are sunny. When the sun stops shining, i’ll write an update to this blog post. Until then, have some cool images.

					</p>
					<h2>References/End Notes</h2>
					<div class="reference">
		                <ol>
		                    <li id="cite_note-4">
		                        <p class="fp"><a href="https://en.wikipedia.org/wiki/Photogrammetry">Photogrammetry</a> is the process of making a 3d model from a set of images.</p>
								<p>Read more: <a href="http://www.theastronauts.com/2014/03/visual-revolution-vanishing-ethan-carter/">Visual Revolution of The Vanishing of Ethan Carter</a></p>
		                    </li>
		                </ol>
					</div>
				
				</div>
			</div>
		</div-->



		<div class="postcontainer">
			<div class="post">
				<div class="post-text">
					<h1>Creating the Davison Building Part 1</h1>
				<!--/div-->
				<a href="../../img/screenshot.jpg">
					<img class="inline-text" src="../../img/screenshot.jpg" alt="Davison Building Render">
				</a>

				<hr>

				<!--div class="post-text"-->
					<h2>Background</h2>
					<p class="fp">
						<span class="sntc">This post is intended to provide an introspection into my workflow when I created the Davison building for the 3D-Atlanta project<sup class="citation" href="#cite_note-1"><a href="#cite_note-1">[1]</a></sup> at Georgia State University.</span> <br> <br>

						<span class="sntc">I selected the Davison building as my first 3D Atlanta model because it was a simple building made with a few repeating elements, and because I was familiar with the building.</span> 
						<span class="sntc">The building is still standing and it's right outside the Peachtree Marta Station.</span> 
						<span class="sntc">I pass by it daily.</span> 
					</p>
					
					<hr>					

					<h2>Gathering Reference</h2>
					<p class="fp">
						<span class="sntc">My SIF coworkers already gathered a lot of great reference from the 1920s, but I also visited the building in real life and shot reference images.</span> 
						<span class="sntc">Our collection of reference images we gathered online left a lot of blind spots, and it didn't capture as much detail as I needed.</span> 
						<span class="sntc">No one in the 1920s wanted to take a photo of the ass-end of the building, so I was lucky enough to be able to go out and do that myself.</span> 
					</p>

					<a href="../../img/reference.jpg">
						<img class="inline-text" src="../../img/reference.jpg" alt="Reference Images">
					</a>

					<p class="fp">
						<span class="sntc">One thing to note is that the building was actually expanded sometime after the 1920s.</span> 
						<span class="sntc">You can see that the side of the building is present in the modern pictures but not in the earlier ones.</span> 
					</p>


					<hr>

					<h2>Blockout</h2>
                    <p class="fp">
						<span class="sntc">The blockout phase<sup class="citation" href="#cite_note-2"><a href="#cite_note-2">[2]</a></sup> is where I spent most of my time when I made the Davison building.</span> 
						<span class="sntc">There's a few things that I tried to figure out in this phase: how to break building down into modular pieces, how to align the models to the grid, and how to match the models to the reference.</span> 
						<span class="sntc">To align the blockout to the reference I first set a reference image as a background image in 3ds Max, then I created a camera in the scene and used 3ds Max's perspective matching tool to align the camera's view to the reference picture I chose.</span>
					</p>
					
					<a href="../../img/perspective_match.jpg">
						<img class="inline-text" src="../../img/perspective_match.jpg" alt="Perspective Matching to the Reference">
					</a>

					<p class="fp">
						<span class="sntc">Next, I placed a box primitive in the scene that was roughly the size of the building.</span> 
						<span class="sntc">I played around with dividing the box primitive into walls and modular windows, and I focused on finding a good size for the windows that would match both the reference and the grid.</span> 
					</p>

					<a href="../../img/window.jpg">
						<img class="inline-text" src="../../img/window.jpg" alt="Matching the models to the reference and the grid">
					</a>

					<p class="fp">	
						<span class="sntc">I settled on making the windows 512 units wide and 576 units tall.</span> 
						<span class="sntc">This dictated the size of the rest of my models, as I aligned almost everything to 512x512 squares.</span> 
						<span class="sntc">After I was satisfied with that, I finished blocking out the rest of the building.</span>	
					</p>

					<a href="../../img/blockout_finished.jpg">
						<img class="inline-text" src="../../img/blockout_finished.jpg" alt="Finishing up the blockout">
					</a>

					<hr>

					<h2>Detailing</h2>
					<p class="fp">
						<span class="sntc">After blocking out the scene, I started to detail the scene.</span> 
						<span class="sntc">At this stage, I didn't care about the poly count of the models, as I was going for the maximum amount of detailing possible.</span> 
						<span class="sntc">I was planning on either replacing the higher poly models or baking the detail into textures.</span>
						<span class="sntc">For this phase, I used a lot of procedural modeling tools such as the Sweep Profile<sup class="citation" href="#cite_note-3"><a href="#cite_note-3">[3]</a></sup> plugin.</span>
					</p>

					<a href="../../img/detailing_finished.jpg">
						<img class="inline-text" src="../../img/detailing_finished.jpg" alt="Finishing up the blockout">
					</a>

					<p class="fp">
						<span class="sntc">I use the plugin heavily when working with architectural pieces, as it's good for breaking up building designs.</span> 
						<span class="sntc">And, since it's a spline modifier, it's easy to jump back and forth between the spline and the high poly, detailed model.</span> 
					</p>
					
					<a href="../../img/molding.jpg">
						<img class="inline-text" src="../../img/molding.jpg" alt="Molding details">
					</a>

					<hr>
					<h2>Part 2</h2>
					<p class="fp">
						<span class="sntc">In the next part, I explain my modular texturing workflow, which shows how I worked from a texture atlas to create highly detailed models and allow high texture reuse.</span> <br><br>
						<span class="sntc">Coming soon...</span>
					</p>

					<hr>

                    <h2>References/End Notes</h2>
					<div class="reference">
		                <ol>
		                    <li id="cite_note-1">
		                        <a href="http://sites.gsu.edu/innovation/2016/04/06/3d-atlanta/">GSU 3D-Atlanta Project</a>
		                    </li>

							<li id="cite_note-2">
		                        <div>
									<p class="fp"><span class="sntc">The blockout phase is (usually) the first phase in 3d modeling.</span> 
									<span class="sntc">It is the phase where the artist works with very simple models or primitives to "block out" the scene.</span></p>
									<p>Read more: <a href="https://www.3dartistonline.com/news/2014/11/maya-tutorial-block-out-3d-scenes/">Maya tutorial: Block out 3D scenes</a></p>
								</div>
		                    </li>

		                    <li id="cite_note-3">
		                        <a href="https://3d-kstudio.com/product/sweep-profile/">Sweep Profile plugin</a>
		                    </li>
		                </ol>
					</div>

				</div> <!-- post text -->
			</div> <!-- post -->
		</div> <!-- container -->

        <script type="text/javascript" src="./js/citation.js"></script>

	</div> <!-- content -->
</body>

</html>
